{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "#os.chdir(\"C:/Users/Kyeongjun/Desktop/LG가전데이터\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. calculating expected cooktime of 27 menus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. 27개 메뉴 선택 & 가나다순 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메뉴 선택 기준 : 100회 이상 요리된 메뉴 &10명 이상의 user(device)가 요리한 메뉴\n",
    "oven = pd.read_csv('Oven_cooktimePred.csv', encoding='euc-kr')\n",
    "\n",
    "menu = list(set(oven.Cook_menu)) # 27개\n",
    "menu.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. menu별 expected cooktime 계산(mean, mode, median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, mean, mode, median = [], [], [], []\n",
    "\n",
    "for i in range(27):\n",
    "    df = oven.loc[oven['Cook_menu'] == menu[i],['Cook_menu', 'Cookingtime']]\n",
    "    df1 = pd.DataFrame(df.Cookingtime.value_counts()).sort_index()  \n",
    "    df1 = pd.DataFrame(list(zip(list(df1.index),list(df1.Cookingtime))), columns= ['time', 'count'])\n",
    "    \n",
    "    m1 = round(sum(df1.iloc[:,0]*df1.iloc[:,1])/sum(df1.iloc[:,1]), 2)\n",
    "    m2 = df1.sort_values(by='count').iloc[len(df1)-1,0]\n",
    "    k = sum(df1['count'])/2\n",
    "    o = (k%2 == 0)\n",
    "    df0 = df.sort_values(by='Cookingtime')\n",
    "    df0 = df0.reset_index(drop=True)\n",
    "    if o :\n",
    "        m3 = (df0['Cookingtime'][round(k)] + df0['Cookingtime'][round(k)+1])/2\n",
    "    else :\n",
    "        m3 = df0['Cookingtime'][round(k)]\n",
    "        \n",
    "    name.append(menu[i]); mean.append(m1); mode.append(m2); median.append(m3)\n",
    "\n",
    "ex_cooktime = pd.DataFrame(list(zip(name,mean,mode,median)), columns = ['menu','mean','mode','median'])\n",
    "del m1, m2, m3, df, df1, df0, k, o, mean, mode, median, name, i\n",
    "\n",
    "# ex_cooktime.to_csv('ex_cooktime.csv', header=True, index=False, encoding = 'euc-kr')\n",
    "# saving expected cooktime dataframe of 27 menus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. NN (user charicteristics + menu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. cooktime prediction용 DF 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "oven_c = pd.read_csv('oven_clustering.csv')         # user charicteristics df\n",
    "\n",
    "oven = pd.read_csv('Oven_sample.csv', encoding = 'euc-kr')\n",
    "oven['Cookingtime'] = oven['Cook_hour']*60*60 + oven['Cook_min']*60 + oven['Cook_sec']  # Cookingtime변수 생성 (시,분,초를 합산)\n",
    "oven = oven.loc[oven['EVENT'] == '요리시작',:]                                          # 요리 시작에 관한 데이터만 추출\n",
    "oven = oven.loc[oven['Cook_menu'].isin(menu),]                                          # 27개 메뉴에 관한 데이터만 추출\n",
    "oven = oven.iloc[:,[2,6,12]]                                                            # DVICE_ID, Cook_menu, Cookingtime 변수만 선택\n",
    "\n",
    "ctPred1 = pd.merge(oven_c,oven, left_on = 'DEVICE_ID', right_on = 'DEVICE_ID', how = 'inner')\n",
    "\n",
    "ctPred1.columns = list(ctPred1.columns[:23]) +['Y']\n",
    "\n",
    "# NN에 입력하기 위해 string으로 표현된 Cook_menu를 각각 0~26의 숫자로 대체합니다.\n",
    "for i in range(len(ctPred1)) :\n",
    "    k = ctPred1.iloc[i,22]\n",
    "    num = menu.index(k)\n",
    "    ctPred1.iloc[i,22] = num\n",
    "    \n",
    "del k, num, i\n",
    "\n",
    "ctPred1 = ctPred1.iloc[:,1:]\n",
    "\n",
    "# ctPred1.to_csv('ctPred1.csv', header=True, index=False)\n",
    "# saving dataframe of NN model1\n",
    "\n",
    "del oven"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. NeuralNetwork modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "train, test = train_test_split(ctPred1, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m           \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_spec_from_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_fallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36mtype_spec_from_value\u001b[1;34m(element, use_fallback)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m   raise TypeError(\"Could not build a TypeSpec for %r with type %s\" %\n\u001b[0m\u001b[0;32m    487\u001b[0m                   (element, type(element).__name__))\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not build a TypeSpec for       Cook_hour  AutoCook_freq  SelfCook_freq  Micro_freq  Oven_freq  \\\n7077      58.22             96           1985        1980         79   \n8483     284.40            189            725         456         66   \n9734      62.80            176            521         390        103   \n2351      96.65             41            551         115         20   \n7054      58.22             96           1985        1980         79   \n...         ...            ...            ...         ...        ...   \n8926      89.22            516            596         629         74   \n5576      95.13             68           2024        1874         64   \n2705     132.23            491            447         147        186   \n258       40.27             99            652         637         47   \n423       72.78            149           2216        2028        152   \n\n      Others_freq  Spring_freq  Summer_freq  Fall_freq  Winter_freq  ...  \\\n7077           22         1166          469        446          505  ...   \n8483          392          656           71        187          308  ...   \n9734          204          480           89        128          262  ...   \n2351          457          343          169         80          157  ...   \n7054           22         1166          469        446          505  ...   \n...           ...          ...          ...        ...          ...  ...   \n8926          409          785          144        183          384  ...   \n5576          154         1332          350        410          613  ...   \n2705          605          638          128        172          276  ...   \n258            67          521          114        116          255  ...   \n423           185         1593          324        448          778  ...   \n\n      Lunch_freq  Dinner_freq  afD_freq  Preheat_freq  Clean_freq  \\\n7077         539          639       167             0           4   \n8483         293          352        86             9          60   \n9734         318          263        51            20          14   \n2351         154          121         8             8          52   \n7054         539          639       167             0           4   \n...          ...          ...       ...           ...         ...   \n8926         135          590        48            11          61   \n5576         778          573       231             5          28   \n2705         164          371         2            64          75   \n258          171          306        66             0          25   \n423          702          883       302            63         126   \n\n      Cancle_freq  Restart_freq  MenuSpectrum  ModeSpectrum  Cook_menu  \n7077          331           108            17            10          7  \n8483          113           105            46            17         16  \n9734          268           157            37            17         20  \n2351          189           385            29            16         22  \n7054          331           108            17            10          1  \n...           ...           ...           ...           ...        ...  \n8926          457           465            45            16         24  \n5576          248           128            26            15          8  \n2705          293           453            39            14          8  \n258           105           106            16            11         14  \n423           320           219            19            12          1  \n\n[8070 rows x 22 columns] with type DataFrame",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2b8f5a66f3b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# model fitting & evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# batch_size = 10으로 설정하여 한번에 10개 row씩 학습됩니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m                                                                         \u001b[1;31m# epochs = 30으로 설정하여 train set을 30번 학습시킵니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1132\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1134\u001b[1;33m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[0;32m   1135\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m           \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1138\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1139\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    346\u001b[0m     dataset = tf.data.Dataset.zip((\n\u001b[0;32m    347\u001b[0m         \u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m     ))\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    604\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m     \"\"\"\n\u001b[1;32m--> 606\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, element)\u001b[0m\n\u001b[0;32m   3823\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3824\u001b[0m     \u001b[1;34m\"\"\"See `Dataset.from_tensors()` for details.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3825\u001b[1;33m     \u001b[0melement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3826\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3827\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;31m# the value. As a fallback try converting the value to a tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         normalized_components.append(\n\u001b[1;32m--> 111\u001b[1;33m             ops.convert_to_tensor(t, name=\"component_%d\" % i))\n\u001b[0m\u001b[0;32m    112\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensorSpec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1565\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1566\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    344\u001b[0m                                          as_ref=False):\n\u001b[0;32m    345\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m   \"\"\"\n\u001b[1;32m--> 271\u001b[1;33m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[0;32m    272\u001b[0m                         allow_broadcast=True)\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    281\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf.constant\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m   \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m   \u001b[1;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m   \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "# make NN model\n",
    "NN = keras.Sequential()\n",
    "NN.add(keras.layers.Input(shape = (train.shape[1]-1)))  # 종속변수 Y를 제외한 변수 개수를 input size로 설정\n",
    "NN.add(keras.layers.Dense(11, activation = \"relu\"))     # hidden layer의 node 개수는 input변수의 개수와 output 변수의 개수의 중간 값으로 설정\n",
    "NN.add(keras.layers.Dense(1, activation = None))        # output변수. 연속형이므로 activation function을 설정하지 않습니다.\n",
    "\n",
    "Adam = keras.optimizers.Adam(learning_rate = 0.001)     # optimizer는 Adam, learning_rate는 0.001\n",
    "\n",
    "NN.compile(optimizer = Adam, loss = 'mse', metrics = ['mse'])           # mse를 기준으로 모델을 평가/학습합니다.\n",
    "\n",
    "# model fitting & evaluation\n",
    "NN.fit(train.iloc[:,:-1],train.iloc[:,-1], epochs = 30, batch_size=10)  # batch_size = 10으로 설정하여 한번에 10개 row씩 학습됩니다.\n",
    "                                                                        # epochs = 30으로 설정하여 train set을 30번 학습시킵니다.\n",
    "\n",
    "NN.evaluate(test.iloc[:,:-1], test.iloc[:,-1])                          # test set에서의 성능을 확인입니다.\n",
    "\n",
    "# (730000**(1/2))/60 # 대략 +-14분정도의 차이를 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. NN (user charicteristics + menu + accumulate/recent session variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. 추가 변수 DF 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성 변수 종류\n",
    "# 동작 이전 : 메뉴 요리 횟수, 레인지 평균 요리 시간, 오븐 평균 요리 시간, \n",
    "# 이전 세션 : 메뉴 요리 여부, 요리 시간 \n",
    "# 기타 : 현지 시간(시), 월\n",
    "\n",
    "# 동작 이전 누적 변수 생성\n",
    "oven_av = pd.read_csv('Oven_sample.csv', encoding = 'euc-kr')\n",
    "oven_av['Cookingtime'] = oven_av['Cook_hour']*60*60 + oven_av['Cook_min']*60 + oven_av['Cook_sec']\n",
    "oven_av = oven_av.loc[oven_av['EVENT'] == '요리시작',]\n",
    "oven_av['Micro_t'], oven_av['Oven_t'], oven_av['MenuFreq'] = 0,0,0\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i in list(set(oven_av.DEVICE_ID)) :\n",
    "    df0 = oven_av.loc[oven_av['DEVICE_ID'] == i,:]\n",
    "    df0 = df0.sort_values(by='LOCAL_TIME')\n",
    "    for j in range(df0.shape[0]) :\n",
    "        df1 = df0.copy(deep=True)\n",
    "        df1 = df1.iloc[:j+1,:]\n",
    "        k = df1.loc[df1['Cook_Mode'].isin(['레인지', '레인지 자동']),'Cookingtime']\n",
    "        if len(k) != 0 :\n",
    "            df0.iloc[j,13] = sum(k)/len(k)\n",
    "        k = df1.loc[df1['Cook_Mode'].isin(['오븐', '오븐 자동']),'Cookingtime']\n",
    "        if len(k) != 0 :\n",
    "            df0.iloc[j,14] = sum(k)/len(k)\n",
    "        k = df1.loc[df1['Cook_menu'] == df0.iloc[j,6],'Cookingtime']\n",
    "        df0.iloc[j,15] = len(k)\n",
    "    \n",
    "    df = df.append(df0)\n",
    "\n",
    "oven_av = df.copy(deep=True)\n",
    "\n",
    "del i, j, df0, df1, k, df\n",
    "\n",
    "oven_av = oven_av.loc[oven_av['Cook_menu'].isin(menu),]     # 27개 메뉴 데이터 추출\n",
    "\n",
    "# 이전 세션 변수 생성 (1hour rule)\n",
    "session = pd.read_csv('session.csv', encoding='euc-kr')\n",
    "session['S1'], session['S2'] = 0, 0\n",
    "\n",
    "for i in range(session.shape[0])[1:] :\n",
    "    s = session.loc[session['Session2'] == session.iloc[i,11]-1,:]\n",
    "    session.iloc[i,14] = sum(s['Cook_hour'])*60*60 + sum(s['Cook_min'])*60 + sum(s['Cook_sec'])\n",
    "    session.iloc[i,13] = 1 if (session.iloc[i,5] in list(s.Cook_menu)) else 0\n",
    "\n",
    "session = session.loc[session['Cook_menu'].isin(menu),:]  # 27개 메뉴 데이터 추출\n",
    "\n",
    "del s, i,\n",
    "\n",
    "# 기타 변수까지 포함한 DataFrame 생성\n",
    "oven_av = oven_av.sort_values(by=['DEVICE_ID','LOCAL_TIME'])        # DEVICE_ID, LOCAL_TIME으로 정렬한 뒤, \n",
    "df0 = oven_av.iloc[:,[1,2,6,12,13,14,15]]                           # 변수 선택\n",
    "df0 = df0.reset_index(drop=True)                                    # session 변수와 합칠 때 index별로 concatenate 되지 않도록 index reset\n",
    "session = session.sort_values(by=['DEVICE_ID','LOCAL_TIME'])        # session 데이터도 위와 동일한 작업을 합니다.\n",
    "df1 = session.iloc[:,[1,13,14]]\n",
    "df1 = df1.reset_index(drop=True)\n",
    "\n",
    "df = pd.concat([df0,df1.iloc[:,[1,2]]],axis=1)                      # 동작이전 누적 변수와 이전세션 변수 통합\n",
    "ctPred2 = pd.merge(oven_c,df, left_on = 'DEVICE_ID', right_on = 'DEVICE_ID', how = 'inner')    # 위 통합 데이터와 DEVICE_ID별 특징변수 통합\n",
    "ctPred2 = ctPred2.sort_values(by=['DEVICE_ID','LOCAL_TIME'])        # DEVICE_ID, LOCAL_TIME으로 정렬\n",
    "\n",
    "# ctPred2.to_csv('ctPred2_preprocessing.csv', header=True, index=False, encoding='euc-kr')\n",
    "# saving dataframe of NN model2 (cookmenu가 한글로 표시된 ver)\n",
    "\n",
    "del df0, df1, df\n",
    "\n",
    "# NN에 입력하기 위해 string으로 표현된 Cook_menu를 각각 0~26의 숫자로 대체합니다.\n",
    "for i in range(len(ctPred2)) :\n",
    "    k = ctPred2.iloc[i,23]\n",
    "    num = menu.index(k)\n",
    "    ctPred2.iloc[i,23] = num\n",
    "    \n",
    "del k, num, i\n",
    "\n",
    "ctPred2['time'], ctPred2['month'] = 0, 0\n",
    "\n",
    "for i in range(len(ctPred2)) :\n",
    "    ctPred2.iloc[i,30] = int(ctPred2.iloc[i,22][11:13])   # time 변수 생성\n",
    "    ctPred2.iloc[i,31] = int(ctPred2.iloc[i,22][5:7])     # month 변수 생성\n",
    "    \n",
    "del i, ctPred2['LOCAL_TIME'], ctPred2['DEVICE_ID']\n",
    "\n",
    "# column 순서 변경 (Cookingtime = Y 를 끝으로)\n",
    "cols = ctPred2.columns.tolist()\n",
    "cols = cols[:cols.index('Cookingtime')] + cols[cols.index('Cookingtime')+1:]\n",
    "cols.append('Cookingtime')\n",
    "ctPred2 = ctPred2[cols]\n",
    "\n",
    "cols = ctPred2.columns.tolist()\n",
    "cols[len(cols)-1] = 'Y'\n",
    "ctPred2.columns = cols\n",
    "\n",
    "del cols\n",
    "\n",
    "# ctPred2.to_csv('ctPred2.csv', header=True, index=False)\n",
    "# saving dataframe of NN model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. NeuralNetwork modeling 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "train, test = train_test_split(ctPred2, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make NN model\n",
    "NN = keras.Sequential()\n",
    "NN.add(keras.layers.Input(shape = (train.shape[1]-1)))\n",
    "NN.add(keras.layers.Dense(18, activation = \"relu\"))     # hidden layer의 layer 개수는 2, node 개수는 input변수의 개수와 output 변수의 개수의 1/3, 1/3 지점으로 설정\n",
    "NN.add(keras.layers.Dense(9, activation = \"relu\"))\n",
    "NN.add(keras.layers.Dense(1, activation = None))\n",
    "\n",
    "Adam = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "NN.compile(optimizer = Adam, loss = 'mse', metrics = ['mse'])\n",
    "\n",
    "NN.fit(train.iloc[:,:-1],train.iloc[:,-1], epochs = 70, batch_size=1)\n",
    "\n",
    "NN.evaluate(test.iloc[:,:-1], test.iloc[:,-1])\n",
    "\n",
    "# (400000**(1/2))/60 # 대략 +-10분정도의 차이를 보임\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. NN (user charicteristics + menu + accumulate/recent session variables + recent cooktime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-1. recent cooktime 변수 DF 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oven_rct = pd.read_csv('Oven_sample.csv', encoding='euc-kr')\n",
    "\n",
    "oven_rct['Cookingtime'] = oven_rct['Cook_hour']*60*60 + oven_rct['Cook_min']*60 + oven_rct['Cook_sec']\n",
    "oven_rct = oven_rct.loc[oven_rct['EVENT'] == '요리시작',]\n",
    "oven_rct = oven_rct.loc[oven_rct['Cook_menu'].isin(menu),]\n",
    "\n",
    "\n",
    "oven_rct['recent_ct'] = 0\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i in list(set(oven_rct.DEVICE_ID)) :\n",
    "    df0 = oven_rct.loc[oven_rct['DEVICE_ID'] == i,:]\n",
    "    df0 = df0.sort_values(by='LOCAL_TIME')\n",
    "    for j in range(1,df0.shape[0]) :\n",
    "        df1 = df0.copy(deep=True)\n",
    "        df2 = df1.iloc[:j,:]\n",
    "        k = list(df2.loc[df2['Cook_menu'] == df1.iloc[j,6],'Cookingtime']) if (df1.iloc[j,6] in list(set(df2['Cook_menu']))) else [0]\n",
    "        k = k[len(k)-1]\n",
    "        df0.iloc[j,13] = k\n",
    "    \n",
    "    df = df.append(df0)\n",
    "\n",
    "oven_rct = df.copy(deep=True)\n",
    "oven_rct = oven_rct.reset_index(drop=True)\n",
    "\n",
    "del i, j, df0, df1, k, df, df2\n",
    "\n",
    "df = pd.read_csv('ctPred3_preprocessing.csv', encoding='euc-kr')\n",
    "\n",
    "df = pd.concat([df,oven_rct.iloc[:,-1]],axis=1)\n",
    "ctPred3 = pd.merge(oven_c,df, left_on = 'DEVICE_ID', right_on = 'DEVICE_ID', how = 'inner')\n",
    "ctPred3 = ctPred3.sort_values(by=['DEVICE_ID','LOCAL_TIME'])\n",
    "\n",
    "# menu를 가나다순으로 0-27까지 레이블링\n",
    "for i in range(len(ctPred3.iloc[:,0])) :\n",
    "    k = ctPred3.iloc[i,23]\n",
    "    num = menu.index(k)\n",
    "    ctPred3.iloc[i,23] = num\n",
    "    \n",
    "del k, num, i\n",
    "\n",
    "ctPred3['time'] = 0\n",
    "\n",
    "for i in range(len(ctPred3.iloc[:,0])) :\n",
    "    ctPred3.iloc[i,31] = int(ctPred3.iloc[i,22][11:13])   # time 변수 생성\n",
    "    \n",
    "del i\n",
    "del ctPred3['LOCAL_TIME'], ctPred3['DEVICE_ID']\n",
    "\n",
    "# column 순서 변경 (Cookingtime = Y 를 끝으로)\n",
    "cols = ctPred3.columns.tolist()\n",
    "cols = cols[:cols.index('Cookingtime')] + cols[cols.index('Cookingtime')+1:]\n",
    "cols.append('Cookingtime')\n",
    "ctPred3 = ctPred3[cols]\n",
    "\n",
    "cols = ctPred3.columns.tolist()\n",
    "cols[len(cols)-1] = 'Y'\n",
    "ctPred3.columns = cols\n",
    "\n",
    "del cols\n",
    "\n",
    "# ctPred3.to_csv('ctPred3.csv', header=True, index=False)\n",
    "# saving dataframe of NN model3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-2. NeuralNet modeling3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctPred3 = pd.read_csv('ctPred3.csv')\n",
    "# train_test_split\n",
    "train, test = train_test_split(ctPred3, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make NN model\n",
    "NN = keras.Sequential()\n",
    "NN.add(keras.layers.Input(shape = (train.shape[1]-1)))\n",
    "NN.add(keras.layers.Dense(18, activation = \"relu\"))\n",
    "NN.add(keras.layers.Dense(9, activation = \"relu\"))\n",
    "NN.add(keras.layers.Dense(1, activation = None))\n",
    "\n",
    "Adam = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "NN.compile(optimizer = Adam, loss = 'mse', metrics = ['mse'])\n",
    "\n",
    "NN.fit(train.iloc[:,:-1],train.iloc[:,-1], epochs = 800, batch_size=10)\n",
    "\n",
    "NN.evaluate(test.iloc[:,:-1], test.iloc[:,-1])\n",
    "\n",
    "# (320000**(1/2))/60 # 대략 +-10분정도의 차이를 보임\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. NN by selected variables (selected user charicteristics + accumulate/recent session variables + recent cooktime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctPred4 = ctPred3.iloc[:,[1,2,3,4,5,16,20,21,22,23,24,25,26,27,28,29]]\n",
    "\n",
    "# train_test_split\n",
    "train, test = train_test_split(ctPred4, test_size = 0.2)\n",
    "\n",
    "# make NN model\n",
    "NN = keras.Sequential()\n",
    "NN.add(keras.layers.Input(shape = (train.shape[1]-1)))\n",
    "NN.add(keras.layers.Dense(7, activation = \"relu\"))\n",
    "NN.add(keras.layers.Dense(1, activation = None))\n",
    "\n",
    "Adam = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "NN.compile(optimizer = Adam, loss = 'mse', metrics = ['mse'])\n",
    "\n",
    "NN.fit(train.iloc[:,:-1],train.iloc[:,-1], epochs = 150, batch_size=10)\n",
    "\n",
    "NN.evaluate(test.iloc[:,:-1], test.iloc[:,-1])\n",
    "\n",
    "# (600000**(1/2))/60 # 대략 +-12분정도의 차이를 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. NN by menu (user charicteristics + accumulate/recent session variables + recent cooktime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-1. 요리횟수가 첫번째로 많은 메뉴(군고구마)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctPred5_7 = ctPred3.loc[ctPred3['Cook_menu'] == 7, ctPred3.columns != 'Cook_menu']\n",
    "\n",
    "# train_test_split\n",
    "train, test = train_test_split(ctPred5_7, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make NN model\n",
    "NN = keras.Sequential()\n",
    "NN.add(keras.layers.Input(shape = (train.shape[1]-1)))\n",
    "NN.add(keras.layers.Dense(18, activation = \"relu\"))\n",
    "NN.add(keras.layers.Dense(9, activation = \"relu\"))\n",
    "NN.add(keras.layers.Dense(1, activation = None))\n",
    "\n",
    "Adam = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "NN.compile(optimizer = Adam, loss = 'mse', metrics = ['mse'])\n",
    "\n",
    "NN.fit(train.iloc[:,:-1],train.iloc[:,-1], epochs = 300, batch_size=10)\n",
    "\n",
    "NN.evaluate(test.iloc[:,:-1], test.iloc[:,-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-2. 요리횟수가 두번째로 많은 메뉴(냉동밥데우기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctPred5_12 = ctPred3.loc[ctPred3['Cook_menu'] == 12, ctPred3.columns != 'Cook_menu']\n",
    "\n",
    "# train_test_split\n",
    "train, test = train_test_split(ctPred5_12, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make NN model\n",
    "NN = keras.Sequential()\n",
    "NN.add(keras.layers.Input(shape = (train.shape[1]-1)))\n",
    "NN.add(keras.layers.Dense(18, activation = \"relu\"))\n",
    "NN.add(keras.layers.Dense(9, activation = \"relu\"))\n",
    "NN.add(keras.layers.Dense(1, activation = None))\n",
    "\n",
    "Adam = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "NN.compile(optimizer = Adam, loss = 'mse', metrics = ['mse'])\n",
    "\n",
    "NN.fit(train.iloc[:,:-1],train.iloc[:,-1], epochs = 100, batch_size=10)\n",
    "\n",
    "NN.evaluate(test.iloc[:,:-1], test.iloc[:,-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. mse of naive rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전에 해당 메뉴를 요리한 경험이 있으면 이전 설정시간을,\n",
    "# 아니면 해당 메뉴의 expected cooktime을 추천하는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-1. 모든 메뉴에 대한 naive rule 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.mean((ctPred3['Y'] - ctPred3['recent_ct'])**(2)))**(1/2)/60\n",
    "\n",
    "# naive한 방식으로 모든 메뉴를 예측하면 22분정도 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-2. 개별 메뉴에 대한 naive rule 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 군고구마\n",
    "df = ctPred5_7.copy(deep=True)\n",
    "df.loc[df['recent_ct'] == 0,'recent_ct'] = 1725.72\n",
    "np.mean((df.Y-df.recent_ct)**2)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 냉동밥데우기\n",
    "df = ctPred5_12.copy(deep=True)\n",
    "df.loc[df['recent_ct'] == 0,'recent_ct'] = 186.44\n",
    "np.mean((df.Y-df.recent_ct)**2)/60\n",
    "\n",
    "# naive한 방식으로 예측하면 23분, 15분정도 오차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
